{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project in Optimization 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from numpy.random import default_rng\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.optimize import approx_fprime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating classes \n",
    "\n",
    "Here we create data of random points seperated by a decision boundary at $w=[1,1], b=1$. We use this data to test our solution algorithms and functions. In order to test a larger number of points, simply choose $npoints$ to be the number of points you wish. The plot illustrates the points. Since the number of points assigned to a class is random, keep the same seed to repeat the results from the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w = np.array([1.,1.])\n",
    "b = 1.\n",
    "\n",
    "npoints = 100\n",
    "n_A = np.random.randint(0,npoints)\n",
    "n_B = npoints-n_A\n",
    "margin = 5.e-1\n",
    "listA, listB = TestLinear(w,b,n_A,n_B,margin)\n",
    "\n",
    "x = np.concatenate((np.array(listA),np.array(listB)))\n",
    "\n",
    "y = np.concatenate((np.ones(n_A), -np.ones(n_B)))\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"Data points\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the linear classification\n",
    "\n",
    "Here we test the linear classification, using gradient descent to find the solution. When choosing C too small, the algorithm wont converge, so we havent run it in the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha0 = np.zeros(n_A+n_B)\n",
    "tau = 0.1\n",
    "niter = 1000\n",
    "C = 3\n",
    "G = pairwise_kernels(x, metric = kernal_linear)  \n",
    "\n",
    "alpha = gradient_descent(alpha0, G, y, tau0=tau, niter=niter, C=C, tol=1e-6, projection=projection)\n",
    "\n",
    "w, b = w_b(alpha, y,x ,  C=C)\n",
    "plot_solution(x, y, w, b)\n",
    "\n",
    "\n",
    "C = 45\n",
    "G = pairwise_kernels(x, metric = kernal_linear)  \n",
    "\n",
    "alpha = gradient_descent(alpha0, G, y, tau0=tau, niter=niter, C=C, tol=1e-6, projection=projection)\n",
    "\n",
    "w, b = w_b(alpha, y,x ,  C=C)\n",
    "plot_solution(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we test the gradient descent with a linesearch, which gives the same resulting boundary but with a shorter runtime, as linesearch finds the solution more optimally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "alpha0 = np.zeros(n_A+n_B)\n",
    "tau = 1\n",
    "niter = 1000\n",
    "C = 50\n",
    "G = pairwise_kernels(x, metric = kernal_linear)\n",
    "\n",
    "\n",
    "alpha = gradient_descent(alpha0, G, y, tau0=tau, niter=niter, C=C, tol=1e-7, projection=projection)\n",
    "alphagrad, fks = gradient_descent_linesearch(alpha0, G, y, tau0=tau, niter=niter, C=C, tol=1e-7, project=projection)\n",
    "\n",
    "w, b = w_b(alpha, y,x ,  C=C)\n",
    "plt.title(\"Gradient descent without linesearch\")\n",
    "plot_solution(x, y, w, b)\n",
    "\n",
    "w, b = w_b(alphagrad, y,x ,  C=C)\n",
    "plt.title(\"Gradient descent with linesearch\")\n",
    "plot_solution(x, y, w, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plot_db function to visualize the decision boundary, this is a bit overkill, and takes longer to compute, but gives the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "plot_db(x, y, alpha, ker = kernal_linear, C=C)\n",
    "plot_db(x, y, alphagrad, ker = kernal_linear, C=C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the different kernels\n",
    "\n",
    "Here we plot the different kernels we implemented. It uses the test_kernel function, which computes the w function and plots the resulting decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "niter = 1000\n",
    "C = 5\n",
    "tol = 1e-6\n",
    "\n",
    "alpha0 = np.zeros(n_A+n_B)\n",
    "test_kernel(alpha0, x, y, kernal_gaussian, niter=niter, C=C, tau0=tau, tol=tol)\n",
    "test_kernel(alpha0, x, y, kernal_inv_multiquadratic, niter=niter, C=C, tau0=tau, tol=tol)\n",
    "test_kernel(alpha0, x, y, kernal_laplacian, niter=niter, C=C, tau0=tau, tol=tol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with different intial conditions\n",
    "\n",
    "To confirm that the initial conditions have little to no effect on the end result we tested how the different kernels get effected by alpha0. The plots didnt get effected, but we observe that the number of iterations to convergence was larger. So the solution becomes the same but the convergence time is longer based on the initial guess, which makes sense.\n",
    "\n",
    "We didnt explore this any further, but its worth noting so we can justify using $\\text{alpha}_0 = (0,0, \\dots, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0s = [\n",
    "    (\"alpha0_random\", np.random.rand(n_A + n_B)),\n",
    "    (\"alpha0_random2\", np.random.rand(n_A + n_B) * 20),\n",
    "    (\"alpha0_0\", np.zeros(n_A + n_B)),\n",
    "    (\"alpha0_1\", np.ones(n_A + n_B)),\n",
    "    \n",
    "]\n",
    "\n",
    "kernels= [kernal_linear,kernal_gaussian, kernal_inv_multiquadratic, kernal_laplacian]\n",
    "\n",
    "for ker in kernels:\n",
    "    for name, alpha0 in alpha0s:\n",
    "        print(\"Kernel: \", ker.__name__, \"and Alpha0:\" ,name)\n",
    "        test_kernel(alpha0, x, y, ker, niter=niter, C=C, tau0=tau, tol=tol, plot=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradient descent on the iris dataset\n",
    "\n",
    "Since data isn't "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2][50:150]  # We'll use only the first two features for visualization\n",
    "Y = iris.target[50:150]  \n",
    "Y[Y==1] = -1  # Convert to -1 and 1 for SVM\n",
    "Y[Y==2] = 1\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Set1, s=50, edgecolors='k')\n",
    "plt.title('Iris Dataset (First Two Features)')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.show()\n",
    "\n",
    "alpha0 = np.zeros(len(Y))\n",
    "ker = kernal_gaussian\n",
    "G = pairwise_kernels(X, metric=ker)\n",
    "C= 100\n",
    "\n",
    "for ker in kernels:\n",
    "    print(\"Kernel: \", ker.__name__)\n",
    "    test_kernel(alpha0, X, Y, ker, niter=niter, C=C, tau0=tau, tol=tol)\n",
    "\n",
    "# test_kernel(alpha0, X, Y, ker, niter=niter, C=C, tau0=tau, tol=tol)\n",
    "\n",
    "# alpha, f = gradient_descent_linesearch(alpha0, G, Y, tau0=tau, niter=niter, C=C, L=10, tol=1e-7, project=projection)\n",
    "\n",
    "# plot_db(X, Y, alpha, ker=ker, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bound constrained lagrangian method\n",
    "\n",
    "Here we have tested out the bound constrained lagrangian method, we sadly have a numerical error that we were not able to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(x[0])\n",
    "M = len(x)\n",
    "epsilon = 1e-7\n",
    "\n",
    "C = 10\n",
    "\n",
    "startpunkt = np.ones(d+1+2*M)\n",
    "lambd_0 = np.ones(M)\n",
    "mu_0 = 10\n",
    "tol_1 = 1e-7\n",
    "tol_2 = 1e-7\n",
    "maxiter = 1000\n",
    "\n",
    "AL_par = [lambd_0, mu_0, d, M, x, y, C]\n",
    "gradAL_par = [lambd_0, mu_0, d, M, x, y, C]\n",
    "constr_par = [x, y]\n",
    "\n",
    "lower_bound = np.append([- np.inf]*(d+1), [0]*2*M)\n",
    "upper_bound = [np.inf]*(d+1+2*M)\n",
    "project_par = [lower_bound, upper_bound]\n",
    "                                         \n",
    "linesearch_par = [lambd_0, mu_0, d, M, x, y, C]\n",
    "\n",
    "BCLM(startpunkt, lambd_0, mu_0, tol_1, tol_2, maxiter, AL, AL_par, constraints, constr_par, grad_AL, gradAL_par, projection_AL, project_par, linesearch_AL, linesearch_par)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
